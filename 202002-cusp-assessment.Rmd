---
title: "Assessment and evaluation tools for the undergraduate statistics major"
author: |
  | Matthew Beckman
  | Penn State University
date: |
  | February 26, 2020
  | University of Minnesota

  
output:
  beamer_presentation: 
    colortheme: seagull
    theme: Pittsburgh
  slidy_presentation: 
    fig_width: 8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(kableExtra)

```

# Collaborators (alphabetical)

- Beth Chance (Cal Poly--San Luis Obispo)
- Kirsten Eilertson (Colorado State)
- Alyssa Hu (Penn State)
- Jennifer Kaplan (Middle Tennessee State)
- Kari Lock Morgan (Penn State)
- Dennis Pearl (Penn State)
- Paul Roback (Saint Olaf College)


# Assessment Outline  

- **Goal: Evaluate learning outcomes of students upon completion of undergraduate statistics program (e.g. major)**
    - snapshot of learning outcomes  
    - cohort comparisons  
    - comprehensive scope
- Constraints
    - faithful to (2014) ASA Curriculum Guidelines[^1]
    - applicable across institutions, instructors, years 
- Status
    - two assessment tools to capture student data
        - indirect & direct assessment
        - multi-year pilot data collection ongoing
        - promising psychometric quality
    - faculty survey for triangulation

[^1]: American Statistical Association Undergraduate Guidelines Workgroup (2014). 2014 Curriculum guidelines for undergraduate programs in statistical science. Alexandria, VA: American Statistical Association. http://www.amstat.org/education/curriculumguidelines.cfm


# (2014) ASA Guidelines for Undergraduate Programs in Statistical Sciences

![](ASA Guidelines.png){ width=95% }


# Comprehensive Undergraduate Statistics Program (CUSP) Assessment Strategy

- [Test Blueprint (Link)](https://bit.ly/2lGxurK)
- 95 competencies cited in 2014 ASA Guidelines  
- single assessment tool likely not sufficient

| # Competencies | ASA Guidelines Topic | 
|:--------------:|:-----------|
| 37 |  Statistical Methods & Theory |
| 16 |  Data Wrangling, Computing, & Data Science |
| 11 |  Mathematical Foundations |
| 18 |  Statistical Practice |
| 9  | Problem Solving |
| 4  | Discipline-Specific Knowledge |


# Comprehensive Undergraduate Statistics Program (CUSP) Assessment Map

- Indirect assessment--CUSP Survey
    - inventory of all 95 competancies cited in ASA Guidelines  
    - survey data self-reported by students
    - approx. 10-15 minutes duration
    - single institution w. multiple cohorts
- Direct assessment--CUSP Test
    - selected response test
    - approx. 1 hour duration
    - multiple institutions w. single cohort
    - single institution w. multiple cohorts
- Indirect assessment--Faculty SPECs
    - program emphasis self-reported by faculty 
    - same 95 topics from ASA Guidelines
    - scale: {incidental; T shows; S does; Assessed}

# Indirect assessment--CUSP Survey 

- **Benefits**
    - Easy implementation
    - Can administer multiple times (beginning/middle/end) during program  
    - No problem including topics we don't teach
    - Includes demographics
- **Risks/Issues**
    - Reliability of self-reporting 
    - Over/Underconfidence with content exposure
- [Indirect Assessment (Survey Link)](https://pennstate.qualtrics.com/jfe/form/SV_73utHIevHKBFBiJ)


### Excerpt

![](cuspSurvey.png){ width=80% }

# Example Use 

- Indirect assessment tool (i.e., Survey) administered at key program milestones
    - first-year seminar
    - midpoint course(s)--if possible
    - beginning & end of capstone course
- Informative for annual program evaluation data
    - requires due caution about interpretation (e.g., Sitzman et al., 2010)
    - most effective when corroborated by other tools


# Comprehensive Undergraduate Statistics Program (CUSP) Assessment Map

- *Indirect assessment--CUSP Survey*
- **Next: Direct assessment--CUSP Test**
    - selected response test
    - approx. 1 hour duration
    - multiple institutions w. single cohort
    - single institution w. multiple cohorts
- Indirect assessment--Faculty SPECs
- Future work


# Direct assessment--CUSP Test

- Selected response assessment tool with broad coverage
- 33 tasks; some with multiple parts
    - 9 testlets  
    - 24 conventional MC questions
- several tasks/subtasks assess multiple competancies
    - score adjustment for successive competancies (1, 1/2, 1/4, ...)
    - 86 points possible
- some tasks adpted from other instruments (with permission)
    - 2 from the REGRESS assessment[^2] 
    - 9 from the CAOS assessment[^3]

[^2]: Enders, F. (2013). Do clinical and translational science graduate students understand linear regression? Development and early validation of the REGRESS quiz. *Clinical and Translational Science, 6*(6).  p. 444-451. 

[^3]: delMas, R., Garfield, J., Ooms, A., Chance, B. (2007). Assessing students' conceptual understanding after a first course in statistics.  *Statistics Education Research Journal, 6*. p. 28-58.


# CUSP Test 

- [Instructor Preview (link)](https://pennstate.qualtrics.com/jfe/form/SV_09cemM9ifT96qoZ)
    - **preview is not for classroom use**
    - password protected 

### Excerpt (partial item)

![](cuspTest.png){ width=80% }


# CUSP Test

- **Benefits**
    - test statistical "reflexes" of students
    - built-in "CAOS" subtest
    - objective measure of student competancies
        - for individual students
        - for a cohort of students
        - aggregate useful for program evaluation
    - Easy implementation
- **Risks/Issues**
    - Variable use conditions may jeopardize comparisons
    - Scope constrained by test fatigue
    - Includes topics we don't necessarily teach
    - Longer to implement 



# Example Use Cases

- Penn State
    - Indirect assessment (i.e., survey) administered multiple times
    - Direct assessment (i.e., test) as midterm in capstone course
    - benchmarking student skills and competancies against ASA Guidelines
    - identify & prioritize cohort needs before graduation
    - program feedback & annual evaluation data
- Other Institutions
    - no course credit
    - homework, extra credit, etc
    - resource constraints (or not)


# Preliminary Item Analysis

- Heuristics[^4]
    - Unidimensionality assumed by common methods of assessment evaluation
    - Cronbach alpha (reliability)
    - descrimination > 0.15 preferred 
    - 0.6 < proportion correct < 0.9
- Results
    - PCA evidence supports unidimensionality
    - Cronbach alpha = 0.81
    - 30/33 items with discrimination > 0.15
    - 9/33 items in recommended difficulty range
    - 21/33 items with > 50% correct

[^4]: Haladyna, T. M., & Rodriguez, M. C. (2013). *Developing and validating test items*. Routledge: New York.


# Comprehensive Undergraduate Statistics Program (CUSP) Assessment Map

- *Indirect assessment--CUSP Survey*
- *Direct assessment--CUSP Test*
- **Next: Indirect assessment--Faculty SPECs**
    - program emphasis self-reported by faculty 
    - same 95 topics from ASA Guidelines
    - scale: {incidental; T shows; S does; Assessed}
- Future work

# Faculty Perception of SPECs

- Statistics Program Emphasis and Contents (SPECs)
- Indirect assessment
    - program emphasis self-reported by faculty/administrator 
    - same 95 topics from ASA Guidelines

![](FacultySPECs.png){ width=80% }


# Comprehensive Undergraduate Statistics Program (CUSP) Assessment Map

- *Indirect assessment--CUSP Survey*
- *Direct assessment--CUSP Test*
- *Indirect assessment--Faculty SPECs*
- **Next: Future work**


# Future work

### Shorter term goals

- Post-graduation follow-up for validation evidence
- Link CUSP Survey data to CUSP Test outcomes  
- Streamline logistics for wider implementation 
- Expand item bank for direct assessment

### Longer term goals

- Experimentation with short/long forms
- Alternative or additional tools for more complete alignment to ASA Guidelines


# Acknowledgments

- Advisory input
    - Nick Horton
    - Allan Rossman
- Pilot testers
    - Heather Smith
    - Andrew Schaffner
    - Nicole Lazar
    - Lynne Seymour
    - Paul Roback
    - Kirsten Eilertson
    - Dave Hunter
    - Christian Schmid
    - Daisy Philtron
- Seed funding & support
    - Penn State Center for Excellence in Science Education
    - Jackie Bortiatynski
    - Mary Beth Williams



\titlepage



# Backup slides

### CUSP Test blueprint alignment to ASA Guidelines

![](CUSP_Weights.png){ width=95% }


# Scree plot of CUSP test data

![](Scree_Plot_Dimensionality.png){ width=80% }


# Item discrimination

- Item discrimination < 0.15
    - (21% correct) Validity of models aligned to a study design
    - (3.6% correct) Strategies to maximize likelihood
    - (40% correct) CAOS task about CI interpretation
- Best item discrimination
    - (discrim = 0.59) Probability distributions task 
    - (discrim = 0.50) Histograms & std deviation task
    - (discrim = 0.46) OLS regression assumptions task


![](Q20.png){ width=80% }



